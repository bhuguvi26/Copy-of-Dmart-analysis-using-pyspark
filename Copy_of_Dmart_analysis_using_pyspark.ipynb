{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvMIl7IqkTLlW1vc2eivNu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuguvi26/Copy-of-Dmart-analysis-using-pyspark/blob/main/Copy_of_Dmart_analysis_using_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRCUs3Nq6wtC",
        "outputId": "fa4727a4-f2b5-4d4c-97c7-6eb73b997e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Spark version: 3.5.1\n",
            "‚úÖ Found: /content/Product.csv\n",
            "‚úÖ Found: /content/data/Sales.csv\n",
            "‚úÖ Found: /content/Customer.csv\n",
            "üìå Product Columns: ['product_id', 'category', 'sub_category', 'product_name']\n",
            "üìå Sales Columns: ['order_line', 'order_id', 'order_date', 'ship_date', 'ship_mode', 'customer_id', 'product_id', 'sales', 'quantity', 'discount', 'profit']\n",
            "üìå Customer Columns: ['customer_id', 'customer_name', 'segment', 'age', 'country', 'city', 'state', 'postal_code', 'region']\n",
            "\n",
            "üìä ---- sales_by_category ----\n",
            "+---------------+-----------+\n",
            "|category       |total_sales|\n",
            "+---------------+-----------+\n",
            "|Technology     |836154.03  |\n",
            "|Furniture      |741999.8   |\n",
            "|Office Supplies|719047.03  |\n",
            "+---------------+-----------+\n",
            "\n",
            "\n",
            "üìä ---- top_customer_orders ----\n",
            "+-----------+-------------------+------+\n",
            "|customer_id|customer_name      |orders|\n",
            "+-----------+-------------------+------+\n",
            "|WB-21850   |William Brown      |37    |\n",
            "|JL-15835   |John Lee           |34    |\n",
            "|MA-17560   |Matt Abelman       |34    |\n",
            "|PP-18955   |Paul Prost         |34    |\n",
            "|CK-12205   |Chloris Kastensmidt|32    |\n",
            "|EH-13765   |Edward Hooks       |32    |\n",
            "|SV-20365   |Seth Vernon        |32    |\n",
            "|JD-15895   |Jonathan Doherty   |32    |\n",
            "|ZC-21910   |Zuschuss Carroll   |31    |\n",
            "|EP-13915   |Emily Phan         |31    |\n",
            "+-----------+-------------------+------+\n",
            "\n",
            "\n",
            "üìä ---- avg_discount ----\n",
            "+------------+\n",
            "|avg_discount|\n",
            "+------------+\n",
            "|0.1562      |\n",
            "+------------+\n",
            "\n",
            "\n",
            "üìä ---- unique_products_region ----\n",
            "+-------+--------+\n",
            "|region |products|\n",
            "+-------+--------+\n",
            "|West   |1536    |\n",
            "|East   |1408    |\n",
            "|Central|1316    |\n",
            "|South  |1059    |\n",
            "+-------+--------+\n",
            "\n",
            "\n",
            "üìä ---- profit_by_state ----\n",
            "+------------+------------+\n",
            "|state       |total_profit|\n",
            "+------------+------------+\n",
            "|California  |59398.31    |\n",
            "|New York    |58177.83    |\n",
            "|Washington  |24405.8     |\n",
            "|Texas       |20528.91    |\n",
            "|Pennsylvania|13604.94    |\n",
            "|Georgia     |12781.34    |\n",
            "|Arizona     |9563.2      |\n",
            "|Illinois    |9560.15     |\n",
            "|Wisconsin   |8569.87     |\n",
            "|Michigan    |7752.3      |\n",
            "+------------+------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "üìä ---- top_subcategory_sales ----\n",
            "+------------+-----------+\n",
            "|sub_category|total_sales|\n",
            "+------------+-----------+\n",
            "|Phones      |330007.05  |\n",
            "|Chairs      |328449.1   |\n",
            "|Storage     |223843.61  |\n",
            "|Tables      |206965.53  |\n",
            "|Binders     |203412.73  |\n",
            "|Machines    |189238.63  |\n",
            "|Accessories |167380.32  |\n",
            "|Copiers     |149528.03  |\n",
            "|Bookcases   |114880.0   |\n",
            "|Appliances  |107532.16  |\n",
            "+------------+-----------+\n",
            "\n",
            "\n",
            "üìä ---- avg_age_segment ----\n",
            "+-----------+-------+\n",
            "|segment    |avg_age|\n",
            "+-----------+-------+\n",
            "|Corporate  |44.82  |\n",
            "|Consumer   |44.61  |\n",
            "|Home Office|43.28  |\n",
            "+-----------+-------+\n",
            "\n",
            "\n",
            "üìä ---- orders_by_shipmode ----\n",
            "+--------------+------+\n",
            "|ship_mode     |orders|\n",
            "+--------------+------+\n",
            "|Standard Class|5968  |\n",
            "|Second Class  |1945  |\n",
            "|First Class   |1538  |\n",
            "|Same Day      |543   |\n",
            "+--------------+------+\n",
            "\n",
            "\n",
            "üìä ---- qty_by_city ----\n",
            "+-------------+------+\n",
            "|city         |qty   |\n",
            "+-------------+------+\n",
            "|New York City|3217.0|\n",
            "|Los Angeles  |2756.0|\n",
            "|Philadelphia |2299.0|\n",
            "|San Francisco|1773.0|\n",
            "|Houston      |1425.0|\n",
            "|Seattle      |1371.0|\n",
            "|Chicago      |1153.0|\n",
            "|Columbus     |854.0 |\n",
            "|Aurora       |611.0 |\n",
            "|San Diego    |609.0 |\n",
            "+-------------+------+\n",
            "\n",
            "\n",
            "üìä ---- profit_margin_segment ----\n",
            "+-----------+---------+----------+------+\n",
            "|segment    |profit   |sales     |margin|\n",
            "+-----------+---------+----------+------+\n",
            "|Home Office|60298.68 |429653.15 |0.1403|\n",
            "|Corporate  |91979.13 |706146.37 |0.1303|\n",
            "|Consumer   |134119.21|1161401.34|0.1155|\n",
            "+-----------+---------+----------+------+\n",
            "\n",
            "\n",
            "‚úÖ ALL DONE ‚Äî Output saved in /content/output/\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# ‚úÖ DMART PySpark Project ‚Äî FINAL STABLE VERSION\n",
        "# ============================\n",
        "\n",
        "# 1Ô∏è‚É£ Install Java & PySpark\n",
        "!apt-get install openjdk-11-jdk -qq > /dev/null\n",
        "!pip install -q pyspark==3.5.1 findspark\n",
        "\n",
        "import os, re, shutil\n",
        "from pathlib import Path\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"PATH\"]+= \":/usr/lib/jvm/java-11-openjdk-amd64/bin\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
        "from google.colab import files\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"DMart_PySpark_Final\").getOrCreate()\n",
        "print(\"‚úÖ Spark version:\", spark.version)\n",
        "\n",
        "# 2Ô∏è‚É£ File loader (search or upload)\n",
        "data_dir = Path(\"/content/data\"); data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "def load_or_upload(name):\n",
        "    for p in [\"/content/data\",\"/content\",\"/mnt/data\"]:\n",
        "        file = Path(p)/name\n",
        "        if file.exists():\n",
        "            print(f\"‚úÖ Found: {file}\")\n",
        "            return str(file)\n",
        "    print(f\"üìÇ Upload {name}\")\n",
        "    up = files.upload()\n",
        "    for f,data in up.items():\n",
        "        p = data_dir/f\n",
        "        open(p,\"wb\").write(data)\n",
        "        return str(p)\n",
        "\n",
        "p_prod   = load_or_upload(\"Product.csv\")\n",
        "p_sales  = load_or_upload(\"Sales.csv\")\n",
        "p_cust   = load_or_upload(\"Customer.csv\")\n",
        "\n",
        "df_prod = spark.read.csv(p_prod, header=True, inferSchema=True)\n",
        "df_sales= spark.read.csv(p_sales, header=True, inferSchema=True)\n",
        "df_cust = spark.read.csv(p_cust, header=True, inferSchema=True)\n",
        "\n",
        "# 3Ô∏è‚É£ Normalize column names\n",
        "def norm(df):\n",
        "    return df.toDF(*[re.sub(r'[^A-Za-z0-9]+','_',c.lower()).strip('_') for c in df.columns])\n",
        "\n",
        "df_prod, df_sales, df_cust = map(norm,[df_prod,df_sales,df_cust])\n",
        "\n",
        "print(\"üìå Product Columns:\", df_prod.columns)\n",
        "print(\"üìå Sales Columns:\", df_sales.columns)\n",
        "print(\"üìå Customer Columns:\", df_cust.columns)\n",
        "\n",
        "# 4Ô∏è‚É£ Rename important keys safely\n",
        "def rename(df, options, new):\n",
        "    for o in options:\n",
        "        if o in df.columns: return df.withColumnRenamed(o,new)\n",
        "    return df\n",
        "\n",
        "df_sales = rename(df_sales,[\"product_id\",\"productid\",\"prod_id\"],\"product_id\")\n",
        "df_prod  = rename(df_prod, [\"product_id\",\"productid\",\"prod_id\"],\"product_id\")\n",
        "\n",
        "df_sales = rename(df_sales,[\"customer_id\",\"customerid\"],\"customer_id\")\n",
        "df_cust  = rename(df_cust, [\"customer_id\",\"customerid\"],\"customer_id\")\n",
        "\n",
        "# 5Ô∏è‚É£ Add missing product_id if not present\n",
        "if \"product_id\" not in df_prod.columns:\n",
        "    print(\"‚ö†Ô∏è No product_id found ‚Äî generating surrogate key\")\n",
        "    w = Window.orderBy(F.monotonically_increasing_id())\n",
        "    df_prod = df_prod.withColumn(\"product_id\", F.row_number().over(w).cast(\"string\"))\n",
        "\n",
        "# Cast key fields to string\n",
        "df_sales = df_sales.withColumn(\"product_id\",F.col(\"product_id\").cast(\"string\"))\n",
        "df_prod  = df_prod.withColumn(\"product_id\",F.col(\"product_id\").cast(\"string\"))\n",
        "df_cust  = df_cust.withColumn(\"customer_id\",F.col(\"customer_id\").cast(\"string\"))\n",
        "df_sales = df_sales.withColumn(\"customer_id\",F.col(\"customer_id\").cast(\"string\"))\n",
        "\n",
        "# 6Ô∏è‚É£ Cast numeric fields\n",
        "for c in [\"sales\",\"quantity\",\"profit\",\"discount\"]:\n",
        "    if c in df_sales.columns:\n",
        "        df_sales = df_sales.withColumn(c, F.col(c).cast(\"double\"))\n",
        "\n",
        "# Create unit_price if missing\n",
        "if \"unit_price\" not in df_sales.columns:\n",
        "    df_sales = df_sales.withColumn(\"unit_price\",\n",
        "        F.when(F.col(\"quantity\")>0, F.col(\"sales\")/F.col(\"quantity\"))\n",
        "    )\n",
        "\n",
        "# Convert order_date\n",
        "if \"order_date\" in df_sales.columns:\n",
        "    df_sales = df_sales.withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
        "\n",
        "# Replace null numeric\n",
        "df_sales = df_sales.fillna({\"profit\":0,\"discount\":0})\n",
        "\n",
        "# 7Ô∏è‚É£ Join tables\n",
        "df = df_sales.join(df_prod,\"product_id\",\"left\") \\\n",
        "             .join(df_cust,\"customer_id\",\"left\")\n",
        "\n",
        "df = df.withColumn(\"sales_amount\", F.col(\"quantity\") * F.col(\"unit_price\"))\n",
        "df.createOrReplaceTempView(\"dmart\")\n",
        "\n",
        "# 8Ô∏è‚É£ Run queries & save\n",
        "out = Path(\"/content/output\"); out.mkdir(exist_ok=True)\n",
        "def run(name,sql):\n",
        "    print(f\"\\nüìä ---- {name} ----\")\n",
        "    df_res = spark.sql(sql)\n",
        "    df_res.show(10,False)\n",
        "    tmp = out/name\n",
        "    df_res.coalesce(1).write.mode(\"overwrite\").option(\"header\",True).csv(str(tmp))\n",
        "    shutil.copy(list(tmp.glob(\"part*.csv\"))[0], out/f\"{name}.csv\")\n",
        "\n",
        "queries = {\n",
        "\"sales_by_category\":\"\"\"\n",
        "SELECT coalesce(category,'Unknown') category,\n",
        "       round(sum(sales_amount),2) total_sales\n",
        "FROM dmart GROUP BY category ORDER BY total_sales DESC\"\"\",\n",
        "\n",
        "\"top_customer_orders\":\"\"\"\n",
        "SELECT customer_id, customer_name, count(order_id) orders\n",
        "FROM dmart GROUP BY customer_id, customer_name\n",
        "ORDER BY orders DESC LIMIT 10\"\"\",\n",
        "\n",
        "\"avg_discount\":\"SELECT round(avg(discount),4) avg_discount FROM dmart\",\n",
        "\n",
        "\"unique_products_region\":\"\"\"\n",
        "SELECT region, count(distinct product_id) products\n",
        "FROM dmart GROUP BY region ORDER BY products DESC\"\"\",\n",
        "\n",
        "\"profit_by_state\":\"\"\"\n",
        "SELECT state, round(sum(profit),2) total_profit\n",
        "FROM dmart GROUP BY state ORDER BY total_profit DESC\"\"\",\n",
        "\n",
        "\"top_subcategory_sales\":\"\"\"\n",
        "SELECT coalesce(sub_category,'Unknown') sub_category,\n",
        "       round(sum(sales_amount),2) total_sales\n",
        "FROM dmart GROUP BY sub_category ORDER BY total_sales DESC LIMIT 10\"\"\",\n",
        "\n",
        "\"avg_age_segment\":\"\"\"\n",
        "SELECT segment, round(avg(age),2) avg_age\n",
        "FROM dmart GROUP BY segment\"\"\",\n",
        "\n",
        "# ‚úÖ FIXED shipping mode query (ship_mode)\n",
        "\"orders_by_shipmode\":\"\"\"\n",
        "SELECT coalesce(ship_mode,'Unknown') ship_mode,\n",
        "       count(order_id) orders\n",
        "FROM dmart\n",
        "GROUP BY ship_mode\n",
        "ORDER BY orders DESC\"\"\",\n",
        "\n",
        "\"qty_by_city\":\"\"\"\n",
        "SELECT city, sum(quantity) qty\n",
        "FROM dmart GROUP BY city ORDER BY qty DESC LIMIT 10\"\"\",\n",
        "\n",
        "\"profit_margin_segment\":\"\"\"\n",
        "SELECT segment,\n",
        "round(sum(profit),2) profit,\n",
        "round(sum(sales_amount),2) sales,\n",
        "round(sum(profit)/sum(sales_amount),4) margin\n",
        "FROM dmart GROUP BY segment ORDER BY margin DESC\"\"\"\n",
        "}\n",
        "\n",
        "for n,q in queries.items(): run(n,q)\n",
        "\n",
        "print(\"\\n‚úÖ ALL DONE ‚Äî Output saved in /content/output/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "SiJZhfqd67y0"
      }
    }
  ]
}